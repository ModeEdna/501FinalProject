[
  {
    "objectID": "notebooks/aboutMe.html",
    "href": "notebooks/aboutMe.html",
    "title": "About Me",
    "section": "",
    "text": "Project\nInitially, I was interested in studying the relationships between big-budget marketing campaigns vs no-budget marketing campaigns (e.g. Zara, Tesla, etc.) and their respective effects on revenue, but the data available for such a problem was sparse, so I decided to move on to my current topic of predicting group stage performance for World Cup squads. Not only did I select this topic out of personal interest, but I also believe that having a high-accuracy predictive model would be quite useful to so-called professional analysts and bettors, given that careers are made of properly guessing sports’ results.\n\n\nIdea hesitancy\nAt first, I hadn’t decided on this idea because I was unsure of the data availability for such a question. Initial searches had led me to many websites that promised sports data, but it was all hidden behind paywalls. Moreover, I worried that many of the variables I would be creating could come from a subjective standpoint, rather than an objective one. An example would be: is the team in a difficult group (yes or no)? Having gone ahead with this project, I would like to create a model with an accuracy rate of more than 65%. I think this can be perceived as an unambitious goal, but I am new to the data science world, so anything substantially better than a 50-50 guess would feel like a realtive success to me. Sadly, the World Cup won’t be over before the semester finishes, so I won’t be able to compare my model’s results to the real world, but I will try to take a look at it once the tournament is over to see how my model performs.\n\n\nWhat is data science?\nThe project outline asked us to give an explanation, in our own words, of what we believe data science to be. This is a very pertinent question because I commonly find people to misunderstand the role of a data scientist. For example, the company where I held my previous job greatly struggled to understand the difference between a data analyst, data engineer, and data scientist. They had us performing duties from many different roles believing we had the appropriate experience to do the job. In my eyes, data science is the most mathematical position of the three aforementioned roles. In a general sense, I would explain data science as the use of data and mathematics to find answers to problems and generate predictions that can help in decision-making. Data science can be an effective tool, helping us find solutions and/or discover new sides to a problem we had never before considered. Nonetheless, this is a bit of a blanket statenebt, because once you consider automation technology, image processing, and other more advanced areas, it becomes more challenging to define the whole of data science in 1-2 sentences.\n\n\nTechnical report vs data science story\nWe were also asked to define 3 differences between a technical report and a data science story. I could name various, but the most important differences are accessibility, engagement, and compulsion.\n\nAccessible: a technical report can be difficult to read for non-technical people; it can even be dificult to read for some technical people. A data science story bridges the gap between the article and the readers’ ability to understand the material. With a simplified language, story-driven report, and ample use of visualizations, a data science story can capture the attention of a much wider audience than a technical report.\nEngaging: adding interactive visualizations and using metaphors to explain complicated topics will lead to wider reader acceptance. If you can engage the reader, they will be more likely to pay attention to the story and remember the message/result of the study. On the other hand, a technical report feels more like reading from a textbook.\nCompulsion: a data science story’s message must be compelling. There is no point in creating a story or presentation that doesn’t resonate with the audience. Getting the public’s attention will most likely generate a strong reaction and make your story relevant and memorable."
  },
  {
    "objectID": "notebooks/aboutMe.html#education",
    "href": "notebooks/aboutMe.html#education",
    "title": "Welcome to my page!",
    "section": "Education",
    "text": "Education\n\n2018: Emory University: Business Administration (ISOM and INTL BUS) and French Language and Literature.\n2024: Expected graduation from Georgetown University: Data Science and Analytics."
  },
  {
    "objectID": "notebooks/aboutMe.html#a-few-points-on-this-website",
    "href": "notebooks/aboutMe.html#a-few-points-on-this-website",
    "title": "Welcome to my page!",
    "section": "A few points on this website",
    "text": "A few points on this website\n\nThis page is intended to represent my work in the program.\nThis specific project is from my 501 ANLY course.\nThe project focuses on predicting possible dark horses for the upcoming 2022 world cup."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "World Cup Success",
    "section": "",
    "text": "Education\n\n2018: Emory University: Business Administration (ISOM and INTL BUS) and French Language and Literature.\n2024: Expected graduation from Georgetown University: Data Science and Analytics.\n\n\n\nA few points on this website\n\nThis page is intended to represent my work in the ANLY 501 class (data science and analytics). Throughout the semester, we learnt about different predictive models and data-handling methods that we then put to the test with our project theme in mind.\nMy final project was created with the goal of predicting which teams will make it out of the group stages in the World Cup.\nTo navigate the website, the left-side panel will move you between topics and sub-topics, while the right side panel can help navigate each specific page, showing the contents of each topic.\nThere are two ways to access the code and data for the project. The first one is by clicking on the Github link at the top of the navigation bar. The second one is by choosing the code you’re interested in from the left-side navigation menu and clicking on the link within the page of interest.\nYou can switch between light and dark mode by clicking on the top-left toggle button!"
  },
  {
    "objectID": "notebooks/aboutMe.html#about-me",
    "href": "notebooks/aboutMe.html#about-me",
    "title": "Welcome to my page!",
    "section": "About me",
    "text": "About me\nMy name is Eduardo Armenta (ea795). I am a first-year student in the Data Science and Analytics program at Georgetown University. For my undergrad studies, I went to Emory university, where I got a B.B.A. with specializations in Information Systems & Operations Management and International Business, along with a B.A. in French Language and Literature. My main interest for this project is to explore the differences between marketing campaigns of companies that choose to advertise their products and those who prefer to spend their marketing money on different areas."
  },
  {
    "objectID": "notebooks/aboutThisPage.html",
    "href": "notebooks/aboutThisPage.html",
    "title": "Project Introduction",
    "section": "",
    "text": "Questions to answer\nHow many of the teams’ players play in Europe’s top 5 leagues? What does each team’s historical performances look like? What is the trend for each team’s recent performance? What is the average age for each team? How about for each starting 11? What is the percentage of players per team that are young and don’t play in Europe? Which lineup formations were used? What is the team’s group stage difficulty? What does each team’s bracket look like? How many fans did the team have in the world cup, in person? FIFA Ranking? Expected goals against? Expected goals per match? Average player form per team leading to the world cup? How many coaches has the team had in the previous 4 years? Were their star players injured prior to world cup?"
  },
  {
    "objectID": "notebooks/aboutThisProject.html",
    "href": "notebooks/aboutThisProject.html",
    "title": "Project Introduction",
    "section": "",
    "text": "Questions to answer\nTo help us understand which types of data we’d have to gather to answer our data science question, we were asked to come up with a few more specific questions that we’ll need to answer to get to the main questions. My questions are the following 10:\n\nHow many of the teams’ players play in Europe’s top 5 leagues?\nWhat is the trend for each team’s recent performance?\nWhat is the average age for each team? How about for each starting 11?\nWhat is the percentage of players per team that are young and don’t play in Europe?\nWhich lineup formations were used?\nWhat is the team’s group stage difficulty?\nWhat does each team’s bracket look like?\nHow many fans did the team have in the world cup, in person?\nFIFA Ranking?\nAverage player form per team leading to the world cup?"
  },
  {
    "objectID": "notebooks/dataGathering.html",
    "href": "notebooks/dataGathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "Twitter sentiment\nWhen it come to text data, I decided to search for Tweets regarding the World Cup so I could identify the sentiment toward the event and certain teams. The spin on this data is that teams with higher mentions and better sentiment might have a correlation with team performance. It’s a bit of a stretch, but I had to do it for the sake of the assignment.\nTo get the Twitter data, I used their API, which was way easier to use than Wikipedia’s own API. The code for the parse is available in the project repository, but all I had to do was select a word or phrase I wanted to appear in the text (I chose FIFA World Cup), a date range for the tweets, how many tweets I wanted to get back, and the information I wanted to get for each tweet (e.g. text, favorited, etc.). The Twitter parse code yielded the following table:\n\n\nCode\n# reading in csv generated with the parse code\npd.read_csv('../data/projectData/Tweets.csv').head()\n\n\n\n\n\n\n  \n    \n      \n      text\n      favorited\n      favoriteCount\n      replyToSN\n      created\n      truncated\n      replyToSID\n      id\n      replyToUID\n      statusSource\n      screenName\n      retweetCount\n      isRetweet\n      retweeted\n      longitude\n      latitude\n    \n  \n  \n    \n      0\n      RT mk2club FIFAWorldCup fever is on Combine FI...\n      False\n      0\n      NaN\n      9/14/22 22:20\n      False\n      NaN\n      1.570180e+18\n      NaN\n      <a href=\"http://twitter.com/download/android\" ...\n      yuiwincityx07x\n      1180\n      True\n      False\n      NaN\n      NaN\n    \n    \n      1\n      Where are our FIFAWorldCup Fans?Did you know w...\n      False\n      0\n      NaN\n      9/14/22 22:20\n      True\n      NaN\n      1.570180e+18\n      NaN\n      <a href=\"http://twitter.com/download/iphone\" r...\n      themunchiemafia\n      0\n      False\n      False\n      NaN\n      NaN\n    \n    \n      2\n      Deporfans Qué opinan de las declaraciones de M...\n      False\n      0\n      NaN\n      9/14/22 22:17\n      True\n      NaN\n      1.570170e+18\n      NaN\n      <a href=\"http://twitter.com/download/iphone\" r...\n      Deportrece\n      0\n      False\n      False\n      NaN\n      NaN\n    \n    \n      3\n      RT neymarjr It is time to rock your world qata...\n      False\n      0\n      NaN\n      9/14/22 22:16\n      False\n      NaN\n      1.570170e+18\n      NaN\n      <a href=\"http://twitter.com/download/iphone\" r...\n      KruzelSvetlana\n      478\n      True\n      False\n      NaN\n      NaN\n    \n    \n      4\n      RT neymarjr It is time to rock your world qata...\n      False\n      0\n      NaN\n      9/14/22 22:11\n      False\n      NaN\n      1.570170e+18\n      NaN\n      <a href=\"http://twitter.com/download/android\" ...\n      Gudson118\n      478\n      True\n      False\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\nNext steps\nBoth datasets are in need of cleaning. For the Tweets data, I’ll have to remove variables that aren’t needed (e.g. truncated, created, etc) and I’ll need to clean the text data itself. Certain operations to do on the data would be lowercasing, removing stopwords, vectorizing, etc. For the player data, I will need to clean the date of birth column, ensure that there are no numerical outliers, and check that the club spellings are correct. Later on, I will group the rows by country and year to turn this dataset into one that has data for each country in each World Cup. It’s possible that later on, when I’m using the data for modeling, I’ll realize that I need more data. My guess is that the table format that I intend on having will be good enough to last me throughout the project, but the amount of variables will likely need to change. For now, the data suffices but it might prove to be too small a quantity with which to properly model.\n\n\nClosing thoughts\nAt the time of writing the closing thoughts section, I have completed all models (expect ARM) and I know understand if the data that I first gathered was enough. Much like my prediction, the data turned out to be less than desired. Having less than 200 rows to both train and test with isn’t great for modeling; the more data the better. Unfortunately, the program and the assignments for this final project kept me too busy to have the time to look for more or better data, so I did what I could with what I had. The only data I managed to replace was the Twitter data. I realized that what I had at my disposition wouldn’t work for the text analysis models, so I went ahead and recreated the Twitter parse but with different code. I didn’t include said code in the earlier sections of this page because I already go into detail about it in the model for which the data was used."
  },
  {
    "objectID": "notebooks/rawData.html",
    "href": "notebooks/rawData.html",
    "title": "Raw Data",
    "section": "",
    "text": "The raw data for this project is available in this github repository.\nAlternatively, you can access the full repository with everything needed for this project and webpage by clicking on the Github icon on the top left part of the navigation bar."
  },
  {
    "objectID": "notebooks/cleanData.html",
    "href": "notebooks/cleanData.html",
    "title": "Clean Data",
    "section": "",
    "text": "The cleaned data for this project is available in this github repository.\nAlternatively, you can access the full repository with everything needed for this project and webpage by clicking on the Github icon on the top left part of the navigation bar."
  },
  {
    "objectID": "notebooks/linkToCode.html",
    "href": "notebooks/linkToCode.html",
    "title": "Code",
    "section": "",
    "text": "You can access the code used to create the several models, do EDA, and other project deliverables in this Github repository.\nAlternatively, you can access the full repository with everything needed for this project and webpage by clicking on the Github icon on the top left part of the navigation bar."
  },
  {
    "objectID": "notebooks/clustering.html",
    "href": "notebooks/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Feature data X\nFor information on how the data was gathered, please refer to the data tab on this website. There, you can find information on data gathering, cleaning, etc. For clusterization methods, our feature data would be the same as the data used for all other models, but I have to remove the PastGroup variable, since those are the clusters we are trying to predict.\nAlthough clustering analysis is often used as a data exploration method, separating the vectors into their correct PastGroup label would offer an appropriate predictive model for me. This is because the goal is for my model to predict who makes it out of the group phase, which would be the same as classifying them under one of the two possible groups: 0 (didn’t make it) and 1 (got out of the group stage).\n\n\nCode\n# import data\ndf = pd.read_csv('../data/cleanData/allTables.csv')\ndf.drop(['Unnamed: 0','Country','Group','Year'], axis=1, inplace=True)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      Host\n      PastGroup\n      DifficultGroup\n      RankFIFA\n      QualiStreak\n      Caps\n      eliteClub\n      Age\n    \n  \n  \n    \n      0\n      0\n      1\n      1\n      20\n      2\n      24.000000\n      7\n      26.212717\n    \n    \n      1\n      0\n      0\n      1\n      1\n      2\n      38.304348\n      15\n      27.370376\n    \n    \n      2\n      0\n      1\n      1\n      42\n      1\n      17.826087\n      0\n      24.355427\n    \n    \n      3\n      0\n      0\n      1\n      24\n      1\n      22.869565\n      5\n      25.345519\n    \n    \n      4\n      0\n      1\n      0\n      8\n      7\n      18.043478\n      8\n      25.915279\n    \n  \n\n\n\n\n\n\nModel descriptions\nK-Means\nK-Means is a model that separates n vectors into k clusters by assigning them to the cluster whose prototype (cluster centroid) is most similar to the vector in question. In simpler terms, K-Means attempts to group different rows of data based on their similarity. Hopefully, if the grouping is done correctly, we can learn new patterns and information about our data and how they relate to each other.\nFor this method, we often use the Euclidean distance between points to understand their similarity. The closer the points are to each other, the more similar they are, and vice versa. To optimize this model, we use the elbow method, which is a graph that uses the inertia and/or distortion metrics to evaluate the model’s ability to cluster data into different groups.\nEven though K-Means is a fast and efficient clustering method, it tends to work best with very specific datasets. It doesn’t do well with data that isn’t obviously forming a cluster or data that has unusual shapes (e.g. rings, circles, etc.).\nDBSCAN\nDBSCAN stands for Density Based Clustering of Applications with Noise. This type of model focuses on the density of data around an area and allows for points that are relatively nearby to be classified as different clusters if there happens to be non-dense space between them. For points that have no density around them, they are seen as outliers. This type of model is non-parametric, meaning that all adjustments to its performance come from the user in the form of hyperparameters.\nDBSCANs are best used for non-linear data, as it is able to better capture complicated relationships between data points. In situations where K-Means struggles to properly identify clusters, it would be a good idea to bring in DBSCANs to see if they perform better.\nIn order to find the optimal hyperparameters, just like with K-Means, we can use the Elbow Method or the Silhouette Method. The Elbow Method focuses on the sum of squared distances of samples to their closest cluster center (good would be low inertia and low number of clusters), while the Silhouette Method focuses on the distance between the clusters (the closer the score is to 1, the better the clustering).\nHierarchical Clustering\nHierarchical clustering is not just one type of model, but a type of clustering. This method of clustering doesn’t assume a particular number of groups, rather it creates a tree (dendrogram) of clusters and continues to separate points until each point is seen as its own cluster. Within hierarchical clustering, there are two types of methods: agglomerative and division. Agglomerative starts by making each point a cluster and works its way up to one cluster. Division does the opposite.\nThese types of models are comparable to DBSCANs in terms of performance. They are able to cluster non-linear data with good precision. However, each model’s accuracy can be dependent on the situation. It is best used for EDA and occasionaly as label predictors (depending on your data). To optimize it, we can also use the Elbow and Silhouette Methods.\n\n\nData preprocessing\nFor this model, I had to remove certain non-numerical labels, given that numerical data is needed for clustering. I could’ve turned the string/factor variables into numerical data but they wouldn’t provide much information on the teams’ performance. The removed variables are Country, Year, and Group. Since none of these are seen as likely predictors of a team’s success, I chose to remove them for this modeling section.\nThe data used will be the same as the data previously shown on this page, but I will be removing the PastGroup variable to use later on for evaluation purposes. Also, there is no point in leaving the respective clusters for each data point within their vector! Furthermore, I standardized the data to ensure proper representation in the vector and improve results. Following is the normalized data set.\n\n\nCode\n# separate the data into features and label and normalize\ny = df[['PastGroup']]\nx = df.drop('PastGroup', axis=1)\n\n# standardize the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(x)\nX = pd.DataFrame(X, columns=x.columns, index=x.index)\nX.head()\n\n\n\n\n\n\n  \n    \n      \n      Host\n      DifficultGroup\n      RankFIFA\n      QualiStreak\n      Caps\n      eliteClub\n      Age\n    \n  \n  \n    \n      0\n      -0.197386\n      1.732051\n      -0.206644\n      -0.460516\n      -1.005656\n      0.464009\n      -0.617086\n    \n    \n      1\n      -0.197386\n      1.732051\n      -1.239863\n      -0.460516\n      0.643064\n      2.060600\n      0.403308\n    \n    \n      2\n      -0.197386\n      1.732051\n      0.989715\n      -0.687232\n      -1.717262\n      -0.933008\n      -2.254155\n    \n    \n      3\n      -0.197386\n      1.732051\n      0.010876\n      -0.687232\n      -1.135950\n      0.064862\n      -1.381459\n    \n    \n      4\n      -0.197386\n      -0.577350\n      -0.859203\n      0.673062\n      -1.692206\n      0.663583\n      -0.879257\n    \n  \n\n\n\n\n\n\nModel creation\nK-Means\nFirst we must go through hyperparameter tuning. In order to do so, we use the Elbow Method, which means iterating over different, possible hyperparameters to see which one gives us the best score.\n\n\nCode\n# import required libraries\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\n\n# created a list for each desired metric to later display on a dataframe and plot\nclus = []\ndist = []\nine = []\nfor i in range(1,11):\n    model = KMeans(n_clusters=i, random_state=0).fit(X)\n    ine.append(model.inertia_)\n    dist.append(sum(np.min(cdist(X, model.cluster_centers_,'euclidean'), axis=1)) / X.shape[0]) # used this part of code from a Geeks for Geeks page\n    clus.append(i)\n\n# use the list to create a dataframe with the performance\ndf2 = pd.DataFrame({'Clusters': clus, 'Distortion': dist, 'Inertia': ine})\ndf2\n\n\n\n\n\n\n  \n    \n      \n      Clusters\n      Distortion\n      Inertia\n    \n  \n  \n    \n      0\n      1\n      2.410444\n      1120.000000\n    \n    \n      1\n      2\n      2.098632\n      866.046925\n    \n    \n      2\n      3\n      1.972900\n      713.955895\n    \n    \n      3\n      4\n      1.799369\n      593.695264\n    \n    \n      4\n      5\n      1.648833\n      516.249552\n    \n    \n      5\n      6\n      1.551209\n      465.984073\n    \n    \n      6\n      7\n      1.481509\n      421.136172\n    \n    \n      7\n      8\n      1.428495\n      390.201734\n    \n    \n      8\n      9\n      1.362819\n      360.102228\n    \n    \n      9\n      10\n      1.337178\n      336.328137\n    \n  \n\n\n\n\nElbow plot:\n\n\nCode\n# elbow plot for inertia\nplt.figure(figsize=(10,5))\nsns.lineplot(data=df2, x='Clusters', y='Inertia')\nplt.title('Performance of Clusters (Inertia)', fontsize=16)\nplt.xlabel('Cluster count', fontsize=13)\nplt.ylabel('Inertia', fontsize=13)\nplt.show()\n\n# elbow plot for distortion\nplt.figure(figsize=(10,5))\nsns.lineplot(data=df2, x='Clusters', y='Distortion')\nplt.title('Performance of Clusters (Distortion)', fontsize=16)\nplt.xlabel('Cluster count', fontsize=13)\nplt.ylabel('Distortion', fontsize=13)\nplt.show()\n\n\n\n\n\n\n\n\nAccording to the above graphs, the optimal choice of clusters would be at the inflection point of 5 clusters. Now that we know this, we must recreate the model with the optimal number of clusters.\n\n\nCode\n# fit the model using the optimal number of clusters\nmodel = KMeans(n_clusters=5, random_state=0).fit(X)\n# print the inertia for the model and variables used\nprint('Inertia for the 5-cluster model is : ' + str(model.inertia_))\nprint('The variables used for the model were : ' + str(model.feature_names_in_))\n\n\nInertia for the 5-cluster model is : 516.2495520286845\nThe variables used for the model were : ['Host' 'DifficultGroup' 'RankFIFA' 'QualiStreak' 'Caps' 'eliteClub' 'Age']\n\n\nIdeally, we would’ve wanted the clustering method to group the data into two groups, one representing the teams that got into the knockout stage and the other representing the teams that didn’t make it out of the group stage. Unfortunately, the optimal number of clusters is 5, so we now have to figure out what each of them represent.\nDBSCAN\nWe repeat the hyperparameter tuning process used for K-Means, but this time with the silhouette score.\n\n\nCode\n# import desired metrics\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\n\n# create list for iterating values and dataframe\nclus = []\neps = []\nsil = []\nfor i in [1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2]:\n    model = DBSCAN(eps=i, min_samples=3)\n    preds = model.fit_predict(X)\n    eps.append(i)\n    sil.append(silhouette_score(X, preds))\n    clus.append(len(pd.Series(preds).unique()))\n\n# create a data frame with the metrics\ndf3 = pd.DataFrame({'Clusters': clus, 'Eps': eps, 'Silhouette': sil})\ndf3\n\n\n\n\n\n\n  \n    \n      \n      Clusters\n      Eps\n      Silhouette\n    \n  \n  \n    \n      0\n      7\n      1.0\n      -0.047503\n    \n    \n      1\n      6\n      1.1\n      0.084714\n    \n    \n      2\n      7\n      1.2\n      0.111169\n    \n    \n      3\n      7\n      1.3\n      0.133401\n    \n    \n      4\n      4\n      1.4\n      0.213129\n    \n    \n      5\n      4\n      1.5\n      0.222293\n    \n    \n      6\n      3\n      1.6\n      0.221920\n    \n    \n      7\n      4\n      1.7\n      0.217839\n    \n    \n      8\n      4\n      1.8\n      0.223472\n    \n    \n      9\n      4\n      1.9\n      0.227048\n    \n    \n      10\n      3\n      2.0\n      0.257012\n    \n  \n\n\n\n\n\n\nCode\n# plot the elbow method graph\nplt.figure(figsize=(10,5))\nsns.lineplot(data=df3, x='Eps',y='Silhouette')\nplt.title('Performance of EPS (Silhouette)', fontsize=16)\nplt.xlabel('Eps value', fontsize=13)\nplt.ylabel('Silhouette score', fontsize=13)\nplt.show()\n\n\n\n\n\nThe optimal results say that we should choose a DBSCAN model with eps of 2. For context, the eps hyperparameter defines the threshold for distance between points to be considered part of the same cluster. That is, for each point that is being compared to another, if they are less than 2 distance units away from each other, they will be considered as belonging to the same group. For the silhouette score, we want to get as close to 1 as possible, and an eps of 2.0 gave us the highest silhouette score of 0.24. This score and eps correspond to a model that created 3 clusters.\n\n\nCode\n# creating the optimal model\nmodel2 = DBSCAN(eps=2, min_samples=3)\npreds = model2.fit_predict(X)\n# printing their labels and variables used\nprint('The labels assigned by this model are : ' + str(set(model2.labels_)))\nprint('The variables used for the model were : ' + str(model2.feature_names_in_))\n\n\nThe labels assigned by this model are : {0, 1, -1}\nThe variables used for the model were : ['Host' 'DifficultGroup' 'RankFIFA' 'QualiStreak' 'Caps' 'eliteClub' 'Age']\n\n\nDBSCAN’s optimal model clusters the data into 3 groups, and most of the points are being put into two of the three groups. This result is closer to what we’re looking for, but we need to see if the clusters correspond to the teams that make it out group stages and those who don’t.\nAgglomerative\nWe repeat the hyperparameter tuning process used for DBSCAN modeling.\n\n\nCode\nfrom sklearn.cluster import AgglomerativeClustering\n# Perform Agglomerative Clustering\nclus = []\nsil = []\nfor i in range(2,11):\n    model = AgglomerativeClustering(n_clusters=i)\n    preds = model.fit_predict(X)\n    sil.append(silhouette_score(X, preds))\n    clus.append(i)\n\n# turn the metrics into a dataframe    \ndf4 = pd.DataFrame({'Clusters': clus, 'Silhouette': sil})\ndf4\n\n\n\n\n\n\n  \n    \n      \n      Clusters\n      Silhouette\n    \n  \n  \n    \n      0\n      2\n      0.236370\n    \n    \n      1\n      3\n      0.268206\n    \n    \n      2\n      4\n      0.277240\n    \n    \n      3\n      5\n      0.228017\n    \n    \n      4\n      6\n      0.243023\n    \n    \n      5\n      7\n      0.236350\n    \n    \n      6\n      8\n      0.238881\n    \n    \n      7\n      9\n      0.243201\n    \n    \n      8\n      10\n      0.236582\n    \n  \n\n\n\n\n\n\nCode\n# plot the iterative process\nplt.figure(figsize=(10,5))\nsns.lineplot(data=df4, x='Clusters',y='Silhouette')\nplt.title('Cluster Count Performance (Silhouette)', fontsize=16)\nplt.xlabel('Cluster count', fontsize=13)\nplt.ylabel('Silhouette score', fontsize=13)\nplt.show()\n\n\n\n\n\nThe optimal results say that we should choose a model with 4 clusters, as the silhouette score is highest with that option. Choosing 3 clusters would yield a similar score, so if we are interested in minimizing the number of clusters, 3 would also be a good option. Since I want to get 2 or 3 clusters, I’ll go for the smaller option.\n\n\nCode\n# creating the optimal model and finding some of its attributes\nmodel3 = AgglomerativeClustering(n_clusters=3)\npreds = model3.fit_predict(X)\n# print the labels and variable used in the model\nprint('The labels assigned by this model are : ' + str(set(model3.labels_)))\nprint('The variables used for the model were : ' + str(model3.feature_names_in_))\n\n\nThe labels assigned by this model are : {0, 1, 2}\nThe variables used for the model were : ['Host' 'DifficultGroup' 'RankFIFA' 'QualiStreak' 'Caps' 'eliteClub' 'Age']\n\n\nThe above code shows some of the attributes coming from the optimal model.\n\n\nResults\nGiven that K-Mean works best for linear data, I found DBSCAN and agglomerative clustering to be the ones that gave the best result, or at least, the results closest to what I was looking for. K-Means wanted to create too many clusters and the other two options decided on a range of 2-4 clusters as the optimal amount. In terms of ease of use, they were all equally easy to create. Since I’m using the sklearn library, all I had to do was replicate the code I had used for the previous model. It only took some messing around with the hyperparameters to create the model.\nTo find connections between the predicted labels and the gorund truth, I added the predictions back into the y dataframe that contained the labels and compared the values for each model’s predictions against the labels.\n\n\nCode\npd.options.mode.chained_assignment = None\n# adding the labels as columns to the y df\ny['K-Means'] = model.labels_\ny['DBSCAN'] = model2.labels_\ny['Agglomerative'] = model3.labels_\ny['K-Means.D'] = np.where(y['PastGroup']==y['K-Means'], True, False)\ny['DBSCAN.D'] = np.where(y['PastGroup']==y['DBSCAN'], True, False)\ny['Agglomerative.D'] = np.where(y['PastGroup']==y['Agglomerative'], True, False)\ny\n\n\n\n\n\n\n  \n    \n      \n      PastGroup\n      K-Means\n      DBSCAN\n      Agglomerative\n      K-Means.D\n      DBSCAN.D\n      Agglomerative.D\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      False\n      False\n      False\n    \n    \n      1\n      0\n      0\n      0\n      0\n      True\n      True\n      True\n    \n    \n      2\n      1\n      6\n      0\n      1\n      False\n      False\n      True\n    \n    \n      3\n      0\n      0\n      0\n      0\n      True\n      True\n      True\n    \n    \n      4\n      1\n      4\n      1\n      0\n      False\n      True\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      155\n      0\n      1\n      1\n      1\n      False\n      False\n      False\n    \n    \n      156\n      1\n      1\n      1\n      1\n      True\n      True\n      True\n    \n    \n      157\n      1\n      1\n      1\n      1\n      True\n      True\n      True\n    \n    \n      158\n      0\n      3\n      1\n      1\n      False\n      False\n      False\n    \n    \n      159\n      0\n      1\n      1\n      1\n      False\n      False\n      False\n    \n  \n\n160 rows × 7 columns\n\n\n\n\n\nCode\n# sum over values to see what got guessed correctly\ny.sum(axis=0)\n\n\nPastGroup           80\nK-Means            409\nDBSCAN             106\nAgglomerative      106\nK-Means.D           32\nDBSCAN.D            71\nAgglomerative.D     51\ndtype: int64\n\n\nI decided to sum the rows to test the accuracy of the clustering methods. Since the length of PastGroup is 160, the closer the sum gets to this number (only works with the columns that end in ‘.D’), the closer we are to good accuracy. We can take the sum of each column and divide by 160 to get the accuracy. Based on this scoring method, the model that performs best is the DBSCAN model, but it still comes in at less than 50%. This is mainly due to the clustering methods creating more than two clusters. So, by default, we will immediately get incorrect clusterization for anything that isn’t a 0 or a 1. This could be fixable by choosing hyperparameters that yield 2 clusters, but then we wouldn’t be selecting the optimal models according to their processes.\n\n\nCode\n# adding the predicted labels to the original df\ndf['K-Means'] = model.labels_\ndf['DBSCAN'] = model2.labels_\ndf['Agglomerative'] = model3.labels_\ndf['K-Means.D'] = np.where(df['PastGroup']==df['K-Means'], True, False)\ndf['DBSCAN.D'] = np.where(df['PastGroup']==df['DBSCAN'], True, False)\ndf['Agglomerative.D'] = np.where(df['PastGroup']==df['Agglomerative'], True, False)\n\n\nTo see if the clustering can give any new information on the data, I plotted a two-dimensional version of the data and colored the points based on their predicte cluster. It can be diffuclt to visualize the reasoning behind the clustering since we’re not including all dimensions used to cluster.\n\n\nCode\n# creating a scatterplot with their predicted labels\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nsns.scatterplot(data=df, x='RankFIFA', y='eliteClub', hue='DBSCAN', ax=ax[0])\nsns.scatterplot(data=df, x='RankFIFA', y='eliteClub', hue='K-Means', ax=ax[1])\nsns.scatterplot(data=df, x='RankFIFA', y='eliteClub', hue='Agglomerative', ax=ax[2])\nax[0].set_title('DBSCAN Cluster Predictions', fontsize=14)\nax[0].set_xlabel('FIFA Rank', fontsize=12)\nax[0].set_ylabel('Elite Club', fontsize=12)\nax[1].set_title('K-Means Cluster Predictions', fontsize=14)\nax[1].set_xlabel('FIFA Rank', fontsize=12)\nax[1].set_ylabel('Elite Club', fontsize=12)\nax[2].set_title('Agglomerative Cluster Predictions', fontsize=14)\nax[2].set_xlabel('FIFA Rank', fontsize=12)\nax[2].set_ylabel('Elite Club', fontsize=12);\n\n\n\n\n\nUnfortunately, the clustering did not provide any new insights on the data. The agglomerative clustering method gets a bit closer to properly separating the clusters than the two other emthods, but it’s still not good enough. From this limited perspective, I theorize that the variables I have for predicting who makes it out of the group stage might not be as effective as initially thought. Either that, or some of the variables aren’t needed and are just creating additional noise.\n\n\nConclusion\nI would like to start my conclusion by reminding the reader that clusterization methods are usually employed for exploratory data analysis. These methods are not meant to be used as predictive models, but, with the right data, they can work well in doing so. After using K-Means, DBSCAN, and Agglomerative clustering methods, optimal models separated the data into 3 or 4 clusters. Although the models are likely finding trends in the data we haven’t been able to decipher, they aren’t keen in clustering the data into only 2 categories.\nIf desired, we could ask the models to purposely put the data into only two clusters, and then we could compare their label predictions against the teams who made it past the group stage and those who didn’t. This would be a next step for this section, but for now I want to accept the recommended optimal models as the ones to analyze.\nIf we’re trying to predict teams’ success in the World Cup with an accuracy of over 70%, these models are not the way to go. The DBSCAN, which has the best score at ~45% accuracy rate, is performing at a worse rate than a random guess. Do keep in mind that the clusters have more than 2 categories, so it makes perfect sense for them to be inefficient in classifying a 2-category problem. Going forward, it could be valuable to explore the relationships that the model found and also their ability to predict the teams that make it out of the group stage. Maybe the model’s ability to predict 1s is much better than the overall accuracy!"
  },
  {
    "objectID": "notebooks/dataCleaning.html",
    "href": "notebooks/dataCleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Cleaning AllSquads table\nTo clean this table, I first checked if there were any NA values in it, which there weren’t. I then cleaned the club column by removing brackets and numbers that appeared in certain cells. After this, I added the eliteClub column, in which the specific player would get a True or False label (0 or 1) based on them playing in one of Europe’s historically elite clubs (i.e. Real Madrid, Manchester United, etc.). I think this could be a useful variable for predicting succes in the World Cup. Lastly, I changed the data type of the date column and other columns read as integers which should instead be categorical values. This is what the table looked like prior to cleaning:\n\nThis is what the table look like after cleaning. We can notice that the player name column is the only one that require further cleaning, but given that I won’t be needing it for any of my analysis, I decided to leave it as is. There is no need to make my workload heavier for variables that won’t be in use.\n\n\n\nCleaning the Qualifying table\nFor the qualifying table cleaning, I first had to merge 5 tables. These tables all contain 4 columns of interest: Team, FIFA Ranking, qualifying streak, and World Cup year. Initially, the tables had more columns than needed and had all the teams that qualified for the given year, so I removed the ones that weren’t important. Then, to merge them, I grabbed the aforementioned columns from each table and renamed them so they all had the same name. This was necessary to ensure that an rbind function call would work. From here, there wasn’t much work to do. I concluded the task by setting variables to their respective type (category, date or integer). Here is what one of the first 5 tables looked like before the merge:\n\nFor the finished product, I got a table without redundant variables and that separates teams into their respective World Cup participation year. This is what the merged table looks like:\n\n\n\nCleaning Countries table\nThis table didn’t require much cleaning, as I created it manually. I ensured there were no unexpected NAs and changed the data type for factor columns that were read as integers. I did, however, have to create columns for this table from a subjective point of view, which could affect future predictions. That is, the column for group of death (difficultGroup) was created based on the professional commentary for each World Cup. The group of death (difficult group column) is the group of highest considered difficulty, but it’s not something that is tangible and can be assigned with rules. The EliteClub column was also created from a subjective point of view. This is a yes-no column based on the club each player plays for, but it’s only a yes if they play in the clubs that are historically considered to be European. elite clubs. The final result for the table is this.\n\n\n\nMerging the three tables\nAfter cleaning those three tables, I went ahead and merged them. In doing so, I first created an Age column for the squads table and substitued any negative values (there were about 30 of them) with the average for the column. I chose to do this instead of deleting the NA rows because the age for participants in the World Cup is strongly concentrated around the 23-33 year range, so the real value won’t be far from the country’s average. Then, I grouped the Squads data by Year and Team to get the mean for Caps, sum for eliteClub, and mean for Age. Once grouped, I was able to merge it with the other tables by Year and Team. The resulting table looks like this.\n\n\n\nCleaning Twitter data\nCleaning the Twitter data took considerable more effort than the previous tables. For this data, I first created a corpus of the tweets by turning the initial dataframe’s text column into a list and then grabbed a set of the list of texts to remove duplicate tweets. I then made lowercased everything, used regex to remove undesired characters like hashtags, links, symbols, etc, which left me with a “clean” set of Tweets. From here, I used CountVectorizer() and recreated part of the code provided for us in HW2.1 and the shared codes folder. This is an image of what the initial data looked like:\n\nI think there is more work to be done. Especially regarding the quality of the Tweets. I say this because the Tweets gathered with Twitter’s API rarely mention the countries participating in the tournamnet. In the next two images, you can see the clean version of the actual Tweets and resulting dataframe after calling the count vectorizer function."
  },
  {
    "objectID": "notebooks/decisiontree.html",
    "href": "notebooks/decisiontree.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Data class distribution\nWe don’t need to dive deep into the data to figure this out. In each World Cup, half of the teams advance and half of the teams get eliminated in the group stage, so there is a perfect balance between labels of making it out of the group stage and not. A perfect means that the tree, or any other model, will weight either label more heavily.\n\n\nDecision tree model\nThe first step is to import the libraries required to work with the data and create the model.\n\n\nCode\n# import required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\n\n\n\nFeature selection and working with the data\nAfter importing the required libraries, I brought in the required dataframe and thought about which features to select. I used the same features utilized for the Naive Bayes predictions. The method used there was to remove variables that had high correlation amongst themselves and I also removed variables that had no correlation whatsoever with the prediction labels. In the same cell, I also normalized the data and separated it into training and testing sets to train the model and later qualify its ability to predict the labels.\n\n\nCode\n# import dataframe\ndf = pd.read_csv('../data/cleanData/allTables.csv')\n# select wanted columns, removed caps variable\ndf2 = df[['eliteClub','RankFIFA','DifficultGroup', 'Host', 'QualiStreak', 'Age']]\nlabel = df[['PastGroup']]\n# normalize data\nscaler = MinMaxScaler()\ndfscaled = scaler.fit_transform(df2)\n# separate data\nX_train, X_test, y_train, y_test = train_test_split(dfscaled, label, test_size=0.25, random_state=42)\n\n\n\n\nDefault tree\nI decided to make an initial attempt with default values for the decision tree to see what the outcome would be. I then created a function that would return the confusion matrix and some metrics (e.g. Accuracy, recall, etc.) to evaluate the tree’s performance. After this, I printed the tree to see a visual representation of how the decisions were being made.\n\n\nCode\n# function to generate a confusion matrix and metrics\ndef confusion_plot(y_data,y_pred):\n    from sklearn.metrics import confusion_matrix ,ConfusionMatrixDisplay\n    cm = confusion_matrix(y_data, y_pred, labels=model.classes_)\n    tn, fp, fn, tp = cm.ravel()\n    print('ACCURACY: ' + str((tn+tp)/(tn+fp+fn+tp)))\n    print('NEGATIVE RECALL (Y=0): ' + str(tn/(tn+fp)))\n    print('NEGATIVE PRECISION (Y=0): ' + str(tn/(tn+fn)))\n    print('POSITIVE RECALL (Y=1): ' + str(tp/(tp+fn)))\n    print('POSITIVE PRECISION (Y=1): ' + str(tp/(tp+fp)))\n    print(np.array([[tn, fp], [fn, tp]]))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n    disp.plot()\n    plt.title(\"Confusion Matrix\", fontsize=16)\n    plt.xlabel('Predicted label', fontsize=13)\n    plt.ylabel('True label', fontsize=12)\n    plt.show()\n\n# create and train the model first attempt\nclf = tree.DecisionTreeClassifier(random_state=42)\nmodel = clf.fit(X_train,y_train)\n# making predictions with the model on the training and the testing data\nyp_train = model.predict(X_train)\nyp_test = model.predict(X_test)\n\n# use the function to predict on the training set\nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\n# use the function to predict on the testing set\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n\n------TRAINING------\nACCURACY: 1.0\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[61  0]\n [ 0 59]]\n\n\n\n\n\n------TEST------\nACCURACY: 0.6\nNEGATIVE RECALL (Y=0): 0.5789473684210527\nNEGATIVE PRECISION (Y=0): 0.5789473684210527\nPOSITIVE RECALL (Y=1): 0.6190476190476191\nPOSITIVE PRECISION (Y=1): 0.6190476190476191\n[[11  8]\n [ 8 13]]\n\n\n\n\n\n\n\nCode\n# visualize model\nfig = plt.figure(figsize=(25,20))\ntree.plot_tree(model, feature_names=df2.columns, class_names=[str(x) for x in label.PastGroup.unique()],filled=True)\nplt.show()\n\n\n\n\n\n\n\nResults for default tree and next step\nGetting an accuracy of 0.6 was somewhat close to a random classifier. That being said, the positive recall, which is the label of interest, had a decent score of 0.62. In an attempt to improve the accuracy of the model, I created a loop to go over various values of maxium depth and compare each depth’s performance on training and testing data.\n\n\nCode\n# looping over hyperparameters and compare\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer, random_state=42)\n    model = model.fit(X_train, y_train)\n\n    yp_train=model.predict(X_train)\n    yp_test=model.predict(X_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label=0),recall_score(y_test, yp_test,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label=0),recall_score(y_train, yp_train,pos_label=1)])\n\n# generate plots to find best hyperparameters\ntest = pd.DataFrame(test_results, columns=['layers', 'accuracy', 'negativeRecall', 'positiveRecall'])\ntrain = pd.DataFrame(train_results, columns=['layers', 'accuracy', 'negativeRecall', 'positiveRecall'])\n\n# plot for accuracy\nplt.figure(figsize=(7,5))\nplt.plot(test['layers'], test['accuracy'], 'r-o')\nplt.plot(train['layers'], train['accuracy'], 'b-o')\nplt.title(\"Iterations Over Layers for Accuracy\", fontsize=16)\nplt.xlabel('Number of layers', fontsize=13)\nplt.ylabel('Accuracy: Train (blue) and Test (red)', fontsize=13)\nplt.show()\n\n# plot for negative recall\nplt.figure(figsize=(7,5))\nplt.plot(test['layers'], test['negativeRecall'], 'r-o')\nplt.plot(train['layers'], train['negativeRecall'], 'b-o')\nplt.title(\"Iterations Over Layers for Negative Recall\", fontsize=16)\nplt.xlabel('Number of layers', fontsize=13)\nplt.ylabel('Neg. Recall: Train (blue) and Test (red)', fontsize=13)\nplt.show()\n\n# plot for positive recall\nplt.figure(figsize=(7,5))\nplt.plot(test['layers'], test['positiveRecall'], 'r-o')\nplt.plot(train['layers'], train['positiveRecall'], 'b-o')\nplt.title(\"Iterations Over Layers for Positive Recall\", fontsize=16)\nplt.xlabel('Number of layers', fontsize=13)\nplt.ylabel('Pos. Recall: Train (blue) and Test (red)', fontsize=13)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal model\nAfter seeing the plots and their respective scores, I selected the max depth with the best metrics and created a decision tree with the optimal parameters.\n\n\nCode\n# retrying the model with the best parameters\nmodel = tree.DecisionTreeClassifier(max_depth=8, random_state=42)\nmodel = model.fit(X_train,y_train)\n# making predictions with the model on the training and the testing data\nyp_train = model.predict(X_train)\nyp_test = model.predict(X_test)\n# testing model\nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n\n------TRAINING------\nACCURACY: 0.9583333333333334\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 0.9242424242424242\nPOSITIVE RECALL (Y=1): 0.9152542372881356\nPOSITIVE PRECISION (Y=1): 1.0\n[[61  0]\n [ 5 54]]\n\n\n\n\n\n------TEST------\nACCURACY: 0.65\nNEGATIVE RECALL (Y=0): 0.6842105263157895\nNEGATIVE PRECISION (Y=0): 0.6190476190476191\nPOSITIVE RECALL (Y=1): 0.6190476190476191\nPOSITIVE PRECISION (Y=1): 0.6842105263157895\n[[13  6]\n [ 8 13]]\n\n\n\n\n\n\n\nCode\n# visualize model\nfig = plt.figure(figsize=(25,20))\ntree.plot_tree(model, feature_names=df2.columns, class_names=[str(x) for x in label.PastGroup.unique()],filled=True)\nplt.show()\n\n\n\n\n\n\n\nBaseline model for comparison\nIn order to compare my predictive model, I created a baseline predictor with a 50-50 chance of generating each label (due to the 50-50 distribution).\n\n\nCode\n# random classifier\nimport numpy as np\nimport random\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\n\ndef generate_label_data(class_labels, weights,N=10000):\n    #e.g. class_labels=[0,1]  weights=[0.2,0.8] (should sum to one)\n    random.seed(42)\n    y=random.choices(class_labels, weights = weights, k = N)\n    print(\"-----GENERATING DATA-----\")\n    print(\"unique entries:\",Counter(y).keys())  \n    print(\"count of labels:\",Counter(y).values()) # counts the elements' frequency\n    print(\"probability of labels:\",np.fromiter(Counter(y).values(), dtype=float)/len(y)) # counts the elements' frequency\n    return y\n\n#TEST\ny=generate_label_data([0,1],[0.5,0.5],40)\n\n# use the confusion plot function we created\nconfusion_plot(y_test,y)\n\n\n-----GENERATING DATA-----\nunique entries: dict_keys([1, 0])\ncount of labels: dict_values([23, 17])\nprobability of labels: [0.575 0.425]\nACCURACY: 0.45\nNEGATIVE RECALL (Y=0): 0.3684210526315789\nNEGATIVE PRECISION (Y=0): 0.4117647058823529\nPOSITIVE RECALL (Y=1): 0.5238095238095238\nPOSITIVE PRECISION (Y=1): 0.4782608695652174\n[[ 7 12]\n [10 11]]\n\n\n\n\n\n\n\nFinal results and conclusion\nThe optimal model did indeed yield better results than both the default decision tree and random classifier. The random classifier had an accuracy of 0.45, while the default tree had an accuracy of 0.60 and the optimal tree generated an accuracy of 0.65. From a general perspective, this means that 65% of the guesses made by the model will be correct and the rest will be incorrect.\nI think the results aren’t great, as I would prefer a model with an accuracy rate above 75%. This model, for now, will have to be qualified as a poor fit. However, we can’t only blame the model for the result. It is likely that there isn’t enough data to properly train the model. Since World Cups occur only every 4 years, we have a limited amount of data. On top of this, collecting the data has been a struggle. A lot of variables have been qualitative and the overall we only have a few variables available to create the model.\nHow can we improve the model and the data? The first step would be to improve the data. I need to find more features to help with the model prediction and I have to make sure they are quantitative, rather than qualitative. If after obtaining new data, the model is still underperforming, then I could try a Random Forest and see if that helps with the result. If it doesn’t, then we can qualify this model as inappropriate for this specific problem and move on to a different classification method.\nAlthough I won’t be able to truly understand the model’s performance until the world cup group stage is over, it will be interesting to find if the model is good at predicting dark horses. That is, how many times that a model predicts an unexpected team to perform well does the team perform well? If this specific accuracy is high, then I would be happy with my model’s results."
  },
  {
    "objectID": "notebooks/SVM.html",
    "href": "notebooks/SVM.html",
    "title": "SVM",
    "section": "",
    "text": "Grabbing data and preparing it for the model\nIn the first few steps, I load the text data that I created during the Naive Baye’s portion of the website and process it as necessary. I make sure to create the labels and to vectorize the text.\n\n\nCode\n# importing some libraries i'll need\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n# load text data\ndf=pd.read_csv('../data/cleanData/wiki-crawl-results.csv')  \nprint(df.shape)\n\n# convert from strings to labels to integers\nlabels=[];\ny1=[]\nfor label in df[\"label\"]:\n    if label not in labels:\n        labels.append(label)\n        print(\"index =\",len(labels)-1,\": label =\",label)\n    for i in range(0,len(labels)):\n        if(label==labels[i]):\n            y1.append(i)\ny1=np.array(y1)\n\n# convert df to list of strings\ncorpus=df[\"text\"].to_list()\ny2=df[\"sentiment\"].to_numpy()\n\n# initialize count vectorizer\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\nvectorizer=CountVectorizer(min_df=0.001)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \nXs = vectorizer.fit_transform(corpus)   \nX = np.array(Xs.todense())\n\n# convert to one-hot vectors\nmaxs = np.max(X,axis=0)\nX = np.ceil(X/maxs)\n\n# split the data\nfrom sklearn.model_selection import train_test_split\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(X, y1, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\n\n(7119, 3)\nindex = 0 : label = FIFA Qatar\nindex = 1 : label = FIFA Ecuador\nindex = 2 : label = FIFA Senegal\nindex = 3 : label = FIFA Netherlands\nindex = 4 : label = FIFA England\nindex = 5 : label = FIFA IR\nindex = 6 : label = FIFA Iran\nindex = 7 : label = FIFA USA\nindex = 8 : label = FIFA Wales\nindex = 9 : label = FIFA Argentina\nindex = 10 : label = FIFA Saudi Arabia\nindex = 11 : label = FIFA Mexico\nindex = 12 : label = FIFA Poland\nindex = 13 : label = FIFA France\nindex = 14 : label = Australia\nindex = 15 : label = FIFA Denmark\nindex = 16 : label = FIFA Tunisia\nindex = 17 : label = FIFA Spain\nindex = 18 : label = FIFA Costa Rica\nindex = 19 : label = FIFA Germany\nindex = 20 : label = FIFA Japan\nindex = 21 : label = FIFA Belgium\nindex = 22 : label = FIFA Canada\nindex = 23 : label = FIFA Morocco\nindex = 24 : label = FIFA Croatia\nindex = 25 : label = FIFA Brazil\nindex = 26 : label = FIFA Serbia\nindex = 27 : label = FIFA Switzerland\nindex = 28 : label = FIFA Cameroon\nindex = 29 : label = FIFA Portugal\nindex = 30 : label = FIFA Ghana\nindex = 31 : label = FIFA Uruguay\nindex = 32 : label = FIFA Korea Republic\n\n\n\n\nClass distribution\nThe class distribution isn’t even. To understand the distribution, I took the count of each label and divided it by the max count of all labels. Ideally, we would want to see percentage values within the 0.90 range, but we have plenty of labels that are at least 4 times smaller in count than the max count label. Having labels that occur more often than others will cause the model to favor those labels, and is likely to misclassify more often than a model with even-weighted labels.\n\n\nCode\n# understand class distribution\nfrom pandas import value_counts\ndaf = pd.DataFrame(value_counts(y1).sort_index())\ndaf['percentage'] = daf[0]/max(daf[0])\ndaf.head(10)\n\n\n\n\n\n\n  \n    \n      \n      0\n      percentage\n    \n  \n  \n    \n      0\n      107\n      0.190053\n    \n    \n      1\n      221\n      0.392540\n    \n    \n      2\n      222\n      0.394316\n    \n    \n      3\n      107\n      0.190053\n    \n    \n      4\n      361\n      0.641208\n    \n    \n      5\n      134\n      0.238011\n    \n    \n      6\n      190\n      0.337478\n    \n    \n      7\n      207\n      0.367673\n    \n    \n      8\n      177\n      0.314387\n    \n    \n      9\n      196\n      0.348135\n    \n  \n\n\n\n\n\n\nFeature selection\nFor this type of model, the features are the texts for the sentiment scores. Feature selection would then come from cleaning the text, which has already been done. I have already removed stop words from the text and did basic text cleaning like stemming, etc. After the cleaning, there is no more feature selection to be done.\n\n\nModel tuning\nFor model tuning, the selection between models would be to figure out which multidimensional method to use for the model. I used the approach of one vs all, but we could’ve used the kernel trick or others. Given that the model takes a long time to converge, it felt unnecessary to create a for loop trying different methods; it would’ve taken hours to run and I would lose too much time doing it. Since my prediction won’t actually use text data and I’m doing this part to complete the homework, I decided to select my first choice as the optimal choice.\n\n\nFinal model\n\n\nCode\nfrom sklearn.svm import SVC\nclf = SVC(decision_function_shape='ovo')\nclf.fit(x_train, y_train)\n\nyt_preds = clf.predict(x_test)\nytr_preds = clf.predict(x_train)\n\n# Display Confusion Matrix for the test data. Remember to use the ConfusionMatrixDisplay function.\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(y_test,yt_preds)\ncmp = ConfusionMatrixDisplay(cm)\nfig, ax = plt.subplots(figsize=(20,20))\nplt.title('Confusion Matrix for Final Model', fontsize=30)\nplt.xlabel('Predicted label', fontsize=22)\nplt.ylabel('True label', fontsize=22)\ncmp.plot(ax=ax);\n\n\n\n\n\n\n\nCode\n# print accuracy score\nprint('Accuracy score for the SVM model is: '+ str(accuracy_score(y_test,yt_preds)))\n\n\nAccuracy score for the SVM model is: 0.3321629213483146\n\n\n\n\nBaseline model for comparison\nIn order to compare my predictive model, I created a baseline predictor with a 1/33 chance of generating each label (due to there being 33 labels).\n\n\nCode\n# random classifier\nimport numpy as np\nimport random\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\n\ndef generate_label_data(class_labels, weights,N=10000):\n    #e.g. class_labels=[0,1]  weights=[0.2,0.8] (should sum to one)\n    random.seed(42)\n    y=random.choices(class_labels, weights = weights, k = N)\n    print(\"-----GENERATING DATA-----\")\n    print(\"unique entries:\",Counter(y).keys())  \n    print(\"count of labels:\",Counter(y).values()) # counts the elements' frequency\n    print(\"probability of labels:\",np.fromiter(Counter(y).values(), dtype=float)/len(y)) # counts the elements' frequency\n    return y\n\n#TEST\ny=generate_label_data([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],[1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33,1/33],1424)\n\n# create the confusion matrix plot\ncm = confusion_matrix(y_test,y)\ncmp = ConfusionMatrixDisplay(cm)\nfig, ax = plt.subplots(figsize=(20,20))\nplt.title('Confusion Matrix for Random Classifier', fontsize=30)\nplt.xlabel('Predicted label', fontsize=22)\nplt.ylabel('True label', fontsize=22)\ncmp.plot(ax=ax);\n\n\n-----GENERATING DATA-----\nunique entries: dict_keys([21, 0, 9, 7, 24, 22, 29, 2, 13, 16, 6, 17, 19, 26, 23, 11, 5, 31, 3, 27, 32, 12, 18, 20, 28, 1, 8, 30, 25, 10, 15, 14, 4])\ncount of labels: dict_values([50, 44, 47, 51, 45, 43, 36, 45, 48, 43, 33, 45, 39, 45, 43, 34, 40, 43, 53, 50, 43, 32, 45, 47, 55, 30, 48, 34, 39, 44, 39, 51, 40])\nprobability of labels: [0.03511236 0.03089888 0.03300562 0.03581461 0.03160112 0.03019663\n 0.0252809  0.03160112 0.03370787 0.03019663 0.02317416 0.03160112\n 0.02738764 0.03160112 0.03019663 0.0238764  0.02808989 0.03019663\n 0.0372191  0.03511236 0.03019663 0.02247191 0.03160112 0.03300562\n 0.0386236  0.02106742 0.03370787 0.0238764  0.02738764 0.03089888\n 0.02738764 0.03581461 0.02808989]\n\n\n\n\n\n\n\nCode\n# print accuracy score\nprint('Accuracy score for the random classifier is: ' + str(accuracy_score(y_test,y)))\n\n\nAccuracy score for the random classifier is: 0.032303370786516857\n\n\n\n\nFinal results and conclusion\nI was very surprised by the SVM’s ability to predict on the test data. With an accuracy score of 0.33, it means that it can correctly predict the text’s label from 33 choices based on the sentiment score of the text. To be fair, I must mention that I didn’t remove partial mentions of the terms within the text, so it affects the accuracy rate. Either way, this is a great score. Compared to the random classifier, it is better by 10-fold.\nFor future steps, I think the first thing to do would be to recreate the model after removing label mentions within the texts, to see if it could accurately predict on their respective labels. I could also try different higher-dimensionality models; I mentioned that I only used one vs all for this problem but I could try the other methods and find out if they yield better results. Furthermore, although out of scope for this class, I could try a neural network to see if it can perform better than the SVM.\nSince my project focuses on using numerical data instead of text data for the predictions, this model won’t transfer well into the goal of this project, but the exercise is good to understand how SVMs work and how they can predict on different dimensions."
  },
  {
    "objectID": "notebooks/conclusion.html",
    "href": "notebooks/conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Data used\nFor the models used, it was best to have a dataset that had a vector for each team, for each World Cup. In other words, I didn’t want to use data based on individual players to make my predictions. To do this, I first had to get data on team squads, including individual player data, to later group it by country + year and get a dataset with the desired format. Numerical data was parsed from Wikipedia and the FIFA website. Text data was parsed from Wikipedia and Twitter.\n\n\nMethods used\nThe course asked us to try out multiple models to see which one would come up with the most accurate predictions. We used Naive Bayes, SVMs, Decision Tress, Clustering and ARM analysis to predict on our training data. After attempting them all, I found that both the Naive Bayes and Decision Tree models performed the best, at around 65% accuracy and recall + precision also oscillating around the 65% range. Since we were asked to use the SVM for text data, the accuracy score for that model cannot be compared to the other two that were used.\n\n\nBest model\nAs mentioned, the best models were Naive Bayes and Decision Trees. The biggest difference between the two, in terms of setting them up, was that the decision tree can use numerical and categorical data that isn’t normalized; Naive Bayes requires normalization and works best with certain types of data. To improve my results, I would likely pursue the decision tree models and explore their efficacy with both more and less variables to see what can lead to an accuracy higher than 75%. I think that the best bet for improving the predictive model is to find more variables, since I only had a handful in my dataset. The lack of variables limited my model’s ability to predict. On top of that, I would also require more data points. Having ~180 at my disposal was not enough to properly train/test the models, and they ended up having biases toward the data shown to them.\n\n\nApplicability\nWhen it comes to applicability, I wouldn’t recommend a professional to use this model… YET. A 65% accuracy rate is respectable, especially seeing how the early stages of the 2022 World Cup have been so unpredictable. However, I’m not satisfied with this model; it still needs to be improved. Once the model is optimized, its applicability would lie in sports analysis programs, sports betting, article writing… basically anything that can use a good prediction on what will happen in a World Cup. With this in mind, I think the best best for a high-accuracy model would be one that’s much more complicated than the methods I attempted.\n\n\nFinal thoughts\nIn my project introduction I mentioned that my real goal for this project is to learn about different models and understand what each one of them is doing. Having gone through the semester, I can now confidently explain what each model is doing and I have a proper understanding of what’s going on behind the scenes. So, in terms of my real goal, I have achieved what I intended. On the other hand, the best model I was able to create left me neither satisfied nor disatisfied; a 65% accuracy performance for an often full-of-surprises tournament isn’t a bad result. That being said, I’m not happy with the model’s accuracy rate. I would like to create a model that can correctly predict who makes it out of the group stage with a 75% accuracy rate. Is it possible? Probably. So I’ll go on experiment to find the optimal model, and once I have it, I would be interested in exploring its weights and studying which variables had the highest weight on the prediction; I would like to know if the variables expected to be important would actually be of importance or if other, less expected variables, have a surprisingly high weight on the predictions."
  },
  {
    "objectID": "notebooks/eda.html",
    "href": "notebooks/eda.html",
    "title": "Exploring Data",
    "section": "",
    "text": "Correlation matrix\nMy first thought was to make a correalation matrix to see if anything moves together with team performance (PastGroup).\n\n\nCode\n# heatmap of correlations\nplt.figure(figsize=(10,7))\nsns.heatmap(all.corr(), annot=True)\nplt.title('Correlation Between Variables', fontsize=22)\nplt.xlabel('Variable', fontsize=16)\nplt.ylabel('Variable', fontsize=16)\nplt.tick_params('both', labelsize=12);\n\n\n\n\n\nInitially, I was searching for strong correlations between my variables and the label feature (PastGroup). I was discouraged when I saw the top 3 correlations being between an absolute value of 0.29 and 0.37. But, after some consideration, I realized that if the correlations were high, there would be no need to create models to see who would perform best. So for the time being, I intend on using those three as predictive variables and continue to look for other features that might have stronger correlations.\n\n\nVariable distributions\nI used the Boxplot to see my summary statistics table in a plot format. This way it would be easier to intuitively understand the numbers.\n\n\nCode\n# create a boxplot of variables used\nfig, ax = plt.subplots(figsize=(11, 7))\nsns.boxplot(data=all[['RankFIFA','QualiStreak','Caps','eliteClub','Age']], ax=ax)\nplt.setp(ax.get_xticklabels(), rotation=90)\nplt.title('Distributions of Variables', fontsize=22)\nax.set_xlabel(\"Feature\", fontsize=16)\nax.set_ylabel(\"Feature Value\", fontsize=16)\nplt.tick_params('both', labelsize=12);\n\n\n\n\n\nThis plot provided me with important information from each variable that isn’t a True or False variable.\nFor RankFIFA, the outliers ended up being the host nations who get automatic qualification into the World Cup. Aside from them, the majority of teams come from the 1-40 rank range, as expected. QualiStreak has an impressive amount of outliers. This is due to the regional powerhouses that almost always qualify to the World Cup. Think Germany, Mexico, France, etc.\nCaps has a dense center, with most teams’ average sitting between 25-40 caps. This makes sense given that outliers on the higher end are those players who were phenoms at a young age and managed to retain their level for a long period of times. A possible next step would be to use the median for these categories rather than the mean. Perhaps certain outliers are influencing the average. Players with 4 or 5 World Cups could easily move the average (Cristiano Ronaldo, Podolski, Memo Ochoa, etc.).\nFor eliteClub, there is one outstanding national team that had 18 out of 23 players playing in the elite clubs. My guess would be that it was either Spain, Germany, or France. It would be interesting to see how far all teams with 10+ players in elite clubs made it in the World Cup.\nThe age variable here isn’t surprising. Given that most soccer players participate in the Cup between the ages of 23 and 33, the compression around the range with almost no outliers is expected. A next step would be to compare the results of the outliers. Would youth or experience yield better results in the tournament?"
  },
  {
    "objectID": "notebooks/NB.html",
    "href": "notebooks/NB.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is one of the simpler techniques for creating classifying models, yet it remains an effective one. It’s common to use this type of model as a first approach due to it’s scalability and ease of use. Like any Naive Bayes model, there is an assumption that each vector is independent and identically distributed. For example, a pet will be considered to be a dog if it weighs over 10 kgs, has colorful fur, and canine teeth; our classifier will assume each variable contributes independently to the probability that this is a dog, irregardless of any possible correlations between the variables.\nThe model uses maximum likelihood estimation (MLE) for the parameters, meaning that we can work with the model without accepting Bayesian classification. Although they seem simple compared to other models, they remain popular due to their surprising effectiveness."
  },
  {
    "objectID": "notebooks/NB.html#naive-bayes-in-python",
    "href": "notebooks/NB.html#naive-bayes-in-python",
    "title": "Naive Bayes",
    "section": "Naive Bayes in Python",
    "text": "Naive Bayes in Python\nThe initial text data that i got, from Twitter, ended up being of low quality, so I decided to utilize the code Dr. Hickman provided for us to get new text data from Wikipedia. The code gathers paragraphs from articles related to desired topics (i.e. ‘FIFA Brazil’) and returns a dataframe text, label, and sentiment. The label will be the topic for the article and the sentiment score comes from the NLTK library, which qualifies its sentiment from -1 to 1 representing negative or positive sentiment. In the second part of the code, the text is vectoried to create a numerical presentation of the text data so that it’s useful as input for the model. The following images show the text data prior to vectorization.\n\nThe second image is a bit difficult to visualize, but it’s a matrix in which each array represents the corpus vocabulary and the zeroes or ones represent if the word from the corpus appears in the provided text. In other words, the columns are each unique word present in the corpus while the rows are yes/no counts of the word appearing in the text.\n\n\nUnderstanding the data\nSince this is new data for the project, I’m adding a small section of data exploration. Given that the dataset is quite simple, we can be very straightforward with the step. If the goal is to classify labels, a pairplot would give us a quick look into possible clusters. Here is the pairplot for the data:\n\nUnfortunately, the Pairplot is showing little to no clusters. This means that the likelihood of our data leading to an accurate model is low. On the other hand, I’m working with 32 labels, so the worst case scenario in terms of accuracy should be 1/32, or 3.16%. Also, although this information isn’t seen through the plot, it must be noted that the label words weren’t removed from the text. For example, if the label is ‘FIFA Germany’, neither of the words were removed from the text, so any accuracy I get can be questions. However, the counterargument here is that the pages gathered from Wikipedia will most likely reference other countries, so the text will have more than one label present in it.\n\n\nModel results\nMy initial attempt at predicting text (we will try other techniques in further homeworks) was with a Naive Bayes multinomial classifier. The results were better than a random classifier, but still not good enough to use in a real-world scenario. In the two following images, we can see the performance of the model on train and test data. We can ignore the result for the train data since we’re only interested in the test data accuracy. For a 32-feature model, getting a 27% accuracy represents a ~9 times improvement over a random classifier, which is a great result. Within the confusion matrix, I find it interesting that the 14th label gets a close to 100% accuracy rate. For the rest of the scores on the diagonal, it seems that only 10 of them are semi-accurately predicting their labels.\n\n\n\n\nConclusions and Discussion\nSo we have the results for this model… what now? How can we tie this into the project’s goal and see if it can help with predicting the outcome of group stages? A far-fetched idea is to get text data from articles found online, predict their label based on their sentiment score, and then assign the sentiment score to the team in the record database. Would sentiment label predict a certain outcome? Maybe the higher the sentiment, the better the expected result for each team. All things taken into consideration, we might have to move on to a different model because this one is only good at predicting the 14th label. Or, if we want to change the script, we could use this model solely for predicting the 14th label and see how well it does. Perhaps it’s great for that.\nNext steps: Firstly, we should remove the label from the text data and see how the model performs without having the labels within the text. Then, we can look at different, possible models that would fit this type of modeling better. A decision tree could be the next step in the process. Based on the results from the record data model, it would seem more appropriate to focus on using data that’s numerical, rather than trying to integrate text data into the problem."
  },
  {
    "objectID": "notebooks/arm.html",
    "href": "notebooks/arm.html",
    "title": "ARM and Networking",
    "section": "",
    "text": "Data used\nThe data I’ll be using comes from the Wikipedia parse I did in one of the other model’s section. To work with it, I’ll have to process it by removing stop words, lemmatizing, stemming, and then tokenizing. For the uninitiated, I’ll explain what each of those terms mean. Stop words are words that carry no importance or significance when it comes to understanding the content of a text (i.e. the, is, are). We want to remove them so we can focus on the text that carries more meaning. Lemmatizing reduces the words to their singular form, so we can analyze them as one item. For example: apples would turn to apple. Stemming reduces the words to their root. So leafs would become leaf and leaves would become leav. And lastly, tokenization is when you reduce your text into the elements of your choice to break text down into understandable parts.\nHere is a reminder of what the text data looks like:\n\n\nCode\n# import required libraries\nimport nltk\nimport string\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom apyori import apriori\nimport networkx as nx\nimport re\n\n# import the file and see the first 5 lines\ntexts = pd.read_csv('../data/cleanData/wiki-crawl-results.csv')\ntexts.head()\n\n\n\n\n\n\n  \n    \n      \n      text\n      label\n      sentiment\n    \n  \n  \n    \n      0\n      south korea japan selected host fifa 31 may 19...\n      FIFA Qatar\n      0.0258\n    \n    \n      1\n      two asian rival went massive expensive pr blit...\n      FIFA Qatar\n      0.6249\n    \n    \n      2\n      first world cup hosted one country second 2026...\n      FIFA Qatar\n      0.9246\n    \n    \n      3\n      199 team attempted qualify 2002 world cup qual...\n      FIFA Qatar\n      0.8807\n    \n    \n      4\n      four nation qualified final first time china e...\n      FIFA Qatar\n      -0.6808\n    \n  \n\n\n\n\n\n\nCode\n# see the distribution of the sentiment\nplt.figure(figsize=(10,7))\nplt.boxplot(texts['sentiment'])\nplt.ylabel('Sentiment score', fontsize=13)\nplt.title('Distribution of Sentiment Among Texts', fontsize=16);\n\n\n\n\n\nWe can appreciate that the average sentiment is quite high (~0.6). It is likely that countries’ feelings toward the World Cup tend to be positive and full of excitement.\n\n\nModel creation\nThe model intakes text data in form of a list of list with individual texts separated into words. I took the Wikipedia texts shown above and put them into the required format to work with the apriori package. From there, I fed the data to the model and reformatted the result into a dataframe with different metrics (see data frame below for reference). Then I used a function from class that converts the data frame into a network and subsequently into a plot of itself. This all leads to the following frame and plot:\n\n\nCode\n# turn sentences into lists of words and put those lists into a list\nsepSent = [text.split() for text in texts['text']]\n\n# results needed for ARM plot\nresults = list(apriori(sepSent, min_support=0.1, min_confidence=0.1, min_length=1, max_length=2))\n\n### These functions are coming from lab 6.1, I did not write most of them\n\n# reformat the apriori output into a df with columns \"rhs\", \"lhs\", \"supp\", \"conf\", \"supp x conf\", \"lift\"\ndef reformat_results(results):\n    keep=[]\n    for i in range(0,len(results)):\n        for j in range(0,len(list(results[i]))):\n            if(j>1):\n                for k in range(0, len(list(results[i][j]))):\n                    if(len(results[i][j][k][0])!=0):\n                        rhs=list(results[i][j][k][0])\n                        lhs=list(results[i][j][k][1])\n                        conf=float(results[i][j][k][2])\n                        lift=float(results[i][j][k][3])\n                        keep.append([rhs,lhs,supp,conf,supp*conf,lift])\n            if(j==1):\n                supp=results[i][j]\n\n    return pd.DataFrame(keep, columns=['rhs','lhs','supp','conf','supp X conf','lift'])\n\ndef convert_to_network(df):\n    print(df)\n\n    #BUILD GRAPH\n    G = nx.DiGraph()  # DIRECTED\n    for row in df.iterrows():\n        # for column in df.columns:\n        lhs=\"_\".join(row[1][0])\n        rhs=\"_\".join(row[1][1])\n        conf=row[1][3]; #print(conf)\n        if(lhs not in G.nodes): \n            G.add_node(lhs)\n        if(rhs not in G.nodes): \n            G.add_node(rhs)\n\n        edge=(lhs,rhs)\n        if edge not in G.edges:\n            G.add_edge(lhs, rhs, weight=conf)\n\n    # print(G.nodes)\n    # print(G.edges)\n    return G\n\ndef plot_network(G):\n    #SPECIFIY X-Y POSITIONS FOR PLOTTING\n    pos=nx.random_layout(G)\n\n    #GENERATE PLOT\n    fig, ax = plt.subplots()\n    fig.set_size_inches(15, 15)\n\n    #assign colors based on attributes\n    weights_e   = [G[u][v]['weight'] for u,v in G.edges()]\n\n    #SAMPLE CMAP FOR COLORS \n    cmap=plt.cm.get_cmap('Blues')\n    colors_e    = [cmap(G[u][v]['weight']*10) for u,v in G.edges()]\n\n    #PLOT\n    nx.draw(\n    G,\n    edgecolors=\"black\",\n    edge_color=colors_e,\n    node_size=2000,\n    linewidths=2,\n    font_size=8,\n    font_color=\"white\",\n    font_weight=\"bold\",\n    width=weights_e,\n    with_labels=True,\n    pos=pos,\n    ax=ax\n    )\n    #ax.set(title='FIFA World Cup')\n    plt.title('Relationships Between Words in Text Data', fontsize=24)\n    plt.show()\n\n# this code plots the resulting relationships\npd_results=reformat_results(results)\nG=convert_to_network(pd_results)\nplot_network(G)\n\n\n         rhs           lhs      supp      conf  supp X conf      lift\n0     [2006]       [world]  0.102683  0.851981     0.087484  1.385711\n1    [world]        [2006]  0.102683  0.167009     0.017149  1.385711\n2     [2010]       [world]  0.110409  0.878212     0.096962  1.428374\n3    [world]        [2010]  0.110409  0.179575     0.019827  1.428374\n4     [also]         [cup]  0.141452  0.581409     0.082242  1.087792\n..       ...           ...       ...       ...          ...       ...\n227  [world]  [tournament]  0.256637  0.417409     0.107123  1.351312\n228    [two]       [world]  0.213794  0.694977     0.148582  1.130350\n229  [world]         [two]  0.213794  0.347727     0.074342  1.130350\n230    [win]       [world]  0.111673  0.727356     0.081226  1.183013\n231  [world]         [win]  0.111673  0.181631     0.020283  1.183013\n\n[232 rows x 6 columns]\n\n\n\n\n\n\n\nResults\nFor our relationship web, we see the most common words from the texts and their relationships to each other. It makes perfect sense that Wikipedia text samples regarding each participating team and their involvemnet in the World Cup would result in seeing words such as: cup, group, host, football, match, etc. It seems that the words fifa, team, world, cup, football, and match are the ones that most often coccur with other words. Although this graphs shows the relationships between words at the most general level, it could be useful to search for the relationships between specific words to understand the sentiment behind certain topics. For example, we could take Mexico and see if it leads us to words such as win or lose.\n\n\nConclusion\nUsing ARM to see how your text data is interconnected can be a fun experiment, and probably a useful one in certain cases. That being said, my goal for this project is to create a model that can predict team performance in the World Cup, so this type of model is not the way to go. Despite this not bein a proper method for my problem, it could be useful for transaction data (as previously mentioned) or perhaps with text from a book. Creating such a plot could lead to understanding the themes from the story and also the words most often associated with certain characters."
  }
]
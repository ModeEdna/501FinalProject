{"title":"Data Gathering","markdown":{"yaml":{"pdf-engine":"lualatex","format":{"html":{"toc":true,"code-fold":true,"toc-title":"Contents","bibliography":"../references.bib","number-sections":true}},"execute":{"echo":true}},"headingText":"Data Gathering","containsRefs":false,"markdown":"\n\n\n### Country squads\n\nTo answer the project questions, I first had to figure out which data to use. After some intial brainstorming, I decided on using tables with squad information such as player experience, player perstige, etc. To do that, I decided to parse Wikipedia for data on each squad member over the past 5 World Cups. With this data at hand, I then would have to find a way to group the teams together by squad and world cup year. To parse Wikipedia for data, I plan on using their API which allows me to get any info desired from the website. This being said, Wikipedia's API is infamously poorly documented, so I could end up turning to different methods (beautiful soup, pandas, etc). The following image shows the table I wanted to grab from Wikipedia.\n\n![](../images/squadTable.jpeg)\n\nAfter grabbing this table, my original worry ended up being correct. I was unable to properly utilize Wikipedia's API and I had to use a combination of Pandas and Beautiful Soup to grab the tables from their respective pages. Additionally, the date of birth column wouldn't properly import, so after much research I ended up using Stack Overflow to solve that specific problem. The end result is the following table:\n\n### Twitter sentiment\n\nWhen it come to text data, I decided to search for Tweets regarding the World Cup so I could identify the sentiment toward the event and certain teams. The spin on this data is that teams with higher mentions and better sentiment might have a correlation with team performance. It's a bit of a stretch, but I had to do it for the sake of the assignment.\n\nTo get the Twitter data, I used their API, which was way easier to use than Wikipedia's own API. The code for the parse is available in the project repository, but all I had to do was select a word or phrase I wanted to appear in the text (I chose FIFA World Cup), a date range for the tweets, how many tweets I wanted to get back, and the information I wanted to get for each tweet (e.g. text, favorited, etc.). The Twitter parse code yielded the following table:\n\n### Next steps\n\nBoth datasets are in need of cleaning. For the Tweets data, I'll have to remove variables that aren't needed (e.g. truncated, created, etc) and I'll need to clean the text data itself. Certain operations to do on the data would be lowercasing, removing stopwords, vectorizing, etc. For the player data, I will need to clean the date of birth column, ensure that there are no numerical outliers, and check that the club spellings are correct. Later on, I will group the rows by country and year to turn this dataset into one that has data for each country in each World Cup. It's possible that later on, when I'm using the data for modeling, I'll realize that I need more data. My guess is that the table format that I intend on having will be good enough to last me throughout the project, but the amount of variables will likely need to change. For now, the data suffices but it might prove to be too small a quantity with which to properly model.\n\n### Closing thoughts\n\nAt the time of writing the closing thoughts section, I have completed all models (expect ARM) and I know understand if the data that I first gathered was enough. Much like my prediction, the data turned out to be less than desired. Having less than 200 rows to both train and test with isn't great for modeling; the more data the better. Unfortunately, the program and the assignments for this final project kept me too busy to have the time to look for more or better data, so I did what I could with what I had. The only data I managed to replace was the Twitter data. I realized that what I had at my disposition wouldn't work for the text analysis models, so I went ahead and recreated the Twitter parse but with different code. I didn't include said code in the earlier sections of this page because I already go into detail about it in the model for which the data was used.\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"pdf-engine":"lualatex","toc":true,"output-file":"dataGathering.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":{"light":"flatly","dark":"darkly"},"title-block-banner":"../images/copa2.jpeg","title-block-banner-color":"white","toc-title":"Contents","bibliography":["../references.bib"]},"extensions":{"book":{"multiFile":true}}}}}
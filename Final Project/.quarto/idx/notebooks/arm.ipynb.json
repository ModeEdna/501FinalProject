{"title":"ARM and Networking","markdown":{"yaml":{"pdf-engine":"lualatex","format":{"html":{"toc":true,"code-fold":true,"toc-title":"Contents","bibliography":"../references.bib","number-sections":true}},"execute":{"echo":true,"warning":false,"message":false}},"headingText":"ARM and Networking","containsRefs":false,"markdown":"\n\n\n### About the model\nAssociation rule mining/learning is used to find relationships of interest among variables in big data sets. We learnt in class that's ARM is most often used for transactional data. To illustrate this further - if we have 10 rows with items separated by commas, ARM will look for patterns of when separate items are bought together, helping us infer the relationships between items. A use case could be: you are trying to market beer to shoppers but don't know where to place it in the store. With a little association rule mining, you learn that beer is often bought with diapers, so you decide to place the beer section closer to the diapers.\n\nFor this project, I won't be using ARM for transaction data. Instead, I will be using it to find relationships between text data. Using ARM, we can find the most often used words and the web that interconnects them all. Although this won't lead to predictions, it can help us understand relationships between words; this could lead to new ways of looking at the content of the book.\n\n### Data used\n\nThe data I'll be using comes from the Wikipedia parse I did in one of the other model's section. To work with it, I'll have to process it by removing stop words, lemmatizing, stemming, and then tokenizing. For the uninitiated, I'll explain what each of those terms mean. Stop words are words that carry no importance or significance when it comes to understanding the content of a text (i.e. the, is, are). We want to remove them so we can focus on the text that carries more meaning. Lemmatizing reduces the words to their singular form, so we can analyze them as one item. For example: apples would turn to apple. Stemming reduces the words to their root. So leafs would become leaf and leaves would become leav. And lastly, tokenization is when you reduce your text into the elements of your choice to break text down into understandable parts.\n\nHere is a reminder of what the text data looks like:\n\nWe can appreciate that the average sentiment is quite high (~0.6). It is likely that countries' feelings toward the World Cup tend to be positive and full of excitement.\n\n### Model creation\n\nThe model intakes text data in form of a list of list with individual texts separated into words. I took the Wikipedia texts shown above and put them into the required format to work with the `apriori` package. From there, I fed the data to the model and reformatted the result into a dataframe with different metrics (see data frame below for reference). Then I used a function from class that converts the data frame into a network and subsequently into a plot of itself. This all leads to the following frame and plot:\n\n### Results\nFor our relationship web, we see the most common words from the texts and their relationships to each other. It makes perfect sense that Wikipedia text samples regarding each participating team and their involvemnet in the World Cup would result in seeing words such as: cup, group, host, football, match, etc. It seems that the words fifa, team, world, cup, football, and match are the ones that most often coccur with other words. Although this graphs shows the relationships between words at the most general level, it could be useful to search for the relationships between specific words to understand the sentiment behind certain topics. For example, we could take Mexico and see if it leads us to words such as win or lose.\n\n### Conclusion\nUsing ARM to see how your text data is interconnected can be a fun experiment, and probably a useful one in certain cases. That being said, my goal for this project is to create a model that can predict team performance in the World Cup, so this type of model is not the way to go. Despite this not bein a proper method for my problem, it could be useful for transaction data (as previously mentioned) or perhaps with text from a book. Creating such a plot could lead to understanding the themes from the story and also the words most often associated with certain characters.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"message":false,"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"pdf-engine":"lualatex","toc":true,"output-file":"arm.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":{"light":"flatly","dark":"darkly"},"title-block-banner":"../images/copa2.jpeg","title-block-banner-color":"white","toc-title":"Contents","bibliography":["../references.bib"]},"extensions":{"book":{"multiFile":true}}}}}
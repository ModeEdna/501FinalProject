{"title":"Conclusion","markdown":{"yaml":{"pdf-engine":"lualatex","format":{"html":{"toc":true,"code-fold":true,"toc-title":"Contents","bibliography":"../references.bib","number-sections":true}},"execute":{"echo":true}},"headingText":"Conclusion","containsRefs":false,"markdown":"\n\n\n### Goal\nMy goal, no pun intended, was to come up with a model that could accurately predict which World Cup teams would make it out of the group stages. My aim was to reach more than 65% accuracy. Although this isn't the most ambitious goal, I was mostly interested in learning about different models and how they work, so I considered a model with a relatively better accuracy than a random prediciton a good result.\n\n### Data used\nFor the models used, it was best to have a dataset that had a vector for each team, for each World Cup. In other words, I didn't want to use data based on individual players to make my predictions. To do this, I first had to get data on team squads, including individual player data, to later group it by country + year and get a dataset with the desired format. Numerical data was parsed from Wikipedia and the FIFA website. Text data was parsed from Wikipedia and Twitter.\n\n### Methods used\nThe course asked us to try out multiple models to see which one would come up with the most accurate predictions. We used Naive Bayes, SVMs, Decision Tress, Clustering and ARM analysis to predict on our training data. After attempting them all, I found that both the Naive Bayes and Decision Tree models performed the best, at around 65% accuracy and recall + precision also oscillating around the 65% range. Since we were asked to use the SVM for text data, the accuracy score for that model cannot be compared to the other two that were used.\n\n### Best model\nAs mentioned, the best models were Naive Bayes and Decision Trees. The biggest difference between the two, in terms of setting them up, was that the decision tree can use numerical and categorical data that isn't normalized; Naive Bayes requires normalization and works best with certain types of data. To improve my results, I would likely pursue the decision tree models and explore their efficacy with both more and less variables to see what can lead to an accuracy higher than 75%. I think that the best bet for improving the predictive model is to find more variables, since I only had a handful in my dataset. The lack of variables limited my model's ability to predict. On top of that, I would also require more data points. Having ~180 at my disposal was not enough to properly train/test the models, and they ended up having biases toward the data shown to them.\n\n### Applicability\nWhen it comes to applicability, I wouldn't recommend a professional to use this model... YET. A 65% accuracy rate is respectable, especially seeing how the early stages of the 2022 World Cup have been so unpredictable. However, I'm not satisfied with this model; it still needs to be improved. Once the model is optimized, its applicability would lie in sports analysis programs, sports betting, article writing... basically anything that can use a good prediction on what will happen in a World Cup. With this in mind, I think the best best for a high-accuracy model would be one that's much more complicated than the methods I attempted.\n\n### Final thoughts\nIn my project introduction I mentioned that my real goal for this project is to learn about different models and understand what each one of them is doing. Having gone through the semester, I can now confidently explain what each model is doing and I have a proper understanding of what's going on behind the scenes. So, in terms of my real goal, I have achieved what I intended. On the other hand, the best model I was able to create left me neither satisfied nor disatisfied; a 65% accuracy performance for an often full-of-surprises tournament isn't a bad result. That being said, I'm not happy with the model's accuracy rate. I would like to create a model that can correctly predict who makes it out of the group stage with a 75% accuracy rate. Is it possible? Probably. So I'll go on experiment to find the optimal model, and once I have it, I would be interested in exploring its weights and studying which variables had the highest weight on the prediction; I would like to know if the variables expected to be important would actually be of importance or if other, less expected variables, have a surprisingly high weight on the predictions.\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"pdf-engine":"lualatex","toc":true,"output-file":"conclusion.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":{"light":"flatly","dark":"darkly"},"title-block-banner":"../images/copa2.jpeg","title-block-banner-color":"white","toc-title":"Contents","bibliography":["../references.bib"]},"extensions":{"book":{"multiFile":true}}}}}
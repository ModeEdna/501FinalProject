{"title":"Decision Trees","markdown":{"yaml":{"pdf-engine":"lualatex","format":{"html":{"toc":true,"code-fold":true,"toc-title":"Contents","bibliography":"../references.bib","number-sections":true}},"execute":{"echo":true,"warning":false}},"headingText":"Decision Trees","containsRefs":false,"markdown":"\n\n\n### Methods\nDecision trees are a great way to classify between two labels (can also be done with more than two). From a general point of view, the tree asks questions and splits data based on the answers. It will continue to ask questions until it perfectly splits the data into their respective labels, so we have to make sure not to overfit. An example: if we were to classify house prices, the first question might be “is the house in an expensive neighborhood?”. This would lead to a split of the data and then the process would continue until it managed to properly classify every datapoint (we are meant to 'prune' the tree so we don't overfit).\n\nThe decision tree is meant to boil things down as much as possible, to a point where all the data is split into their respective category. The problem with this method is that if allowed, it will overfit to the training data and perform poorly on the test data. To fix this, we can specify hyperparameters within the model such as maximum depth (i.e., how many questions the tree can ask), required data instances to ask another question (i.e., how many data instances have to fall under a leaf to ask another question), and a few others.\n\nIn the end, the best result will be one that maximizes the accuracy of both training and test data. If we are maximizing both, it means that the generalization the model is making works both in theory and practice. A next step for this model would be to use a random forest, rather than a decision tree, which creates many decision trees and creates a “vote” of the results to assign a label to your predicted data.\n\n### Data class distribution\nWe don't need to dive deep into the data to figure this out. In each World Cup, half of the teams advance and half of the teams get eliminated in the group stage, so there is a perfect balance between labels of making it out of the group stage and not. A perfect means that the tree, or any other model, will weight either label more heavily.\n\n### Decision tree model\nThe first step is to import the libraries required to work with the data and create the model.\n\n### Feature selection and working with the data\nAfter importing the required libraries, I brought in the required dataframe and thought about which features to select. I used the same features utilized for the Naive Bayes predictions. The method used there was to remove variables that had high correlation amongst themselves and I also removed variables that had no correlation whatsoever with the prediction labels.\nIn the same cell, I also normalized the data and separated it into training and testing sets to train the model and later qualify its ability to predict the labels.\n\n### Default tree\n\nI decided to make an initial attempt with default values for the decision tree to see what the outcome would be. I then created a function that would return the confusion matrix and some metrics (e.g. Accuracy, recall, etc.) to evaluate the tree's performance. After this, I printed the tree to see a visual representation of how the decisions were being made.\n\n### Results for default tree and next step\n\nGetting an accuracy of 0.6 was somewhat close to a random classifier. That being said, the positive recall, which is the label of interest, had a decent score of 0.62. In an attempt to improve the accuracy of the model, I created a loop to go over various values of maxium depth and compare each depth's performance on training and testing data.\n\n### Optimal model\n\nAfter seeing the plots and their respective scores, I selected the max depth with the best metrics and created a decision tree with the optimal parameters.\n\n### Baseline model for comparison\n\nIn order to compare my predictive model, I created a baseline predictor with a 50-50 chance of generating each label (due to the 50-50 distribution).\n\n### Final results and conclusion\n\nThe optimal model did indeed yield better results than both the default decision tree and random classifier. The random classifier had an accuracy of 0.45, while the default tree had an accuracy of 0.60 and the optimal tree generated an accuracy of 0.65. From a general perspective, this means that 65% of the guesses made by the model will be correct and the rest will be incorrect.\n\nI think the results aren't great, as I would prefer a model with an accuracy rate above 75%. This model, for now, will have to be qualified as a poor fit. However, we can't only blame the model for the result. It is likely that there isn't enough data to properly train the model. Since World Cups occur only every 4 years, we have a limited amount of data. On top of this, collecting the data has been a struggle. A lot of variables have been qualitative and the overall we only have a few variables available to create the model.\n\nHow can we improve the model and the data? The first step would be to improve the data. I need to find more features to help with the model prediction and I have to make sure they are quantitative, rather than qualitative. If after obtaining new data, the model is still underperforming, then I could try a Random Forest and see if that helps with the result. If it doesn't, then we can qualify this model as inappropriate for this specific problem and move on to a different classification method.\n\nAlthough I won't be able to truly understand the model's performance until the world cup group stage is over, it will be interesting to find if the model is good at predicting dark horses. That is, how many times that a model predicts an unexpected team to perform well does the team perform well? If this specific accuracy is high, then I would be happy with my model's results.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"pdf-engine":"lualatex","toc":true,"output-file":"decisiontree.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":{"light":"flatly","dark":"darkly"},"title-block-banner":"../images/copa2.jpeg","title-block-banner-color":"white","toc-title":"Contents","bibliography":["../references.bib"]},"extensions":{"book":{"multiFile":true}}}}}
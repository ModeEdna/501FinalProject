{"title":"Clustering","markdown":{"yaml":{"pdf-engine":"lualatex","format":{"html":{"toc":true,"code-fold":true,"toc-title":"Contents","bibliography":"../references.bib","number-sections":true}},"execute":{"echo":true,"warning":false,"message":false}},"headingText":"Clustering","containsRefs":false,"markdown":"\n\n\n### Feature data X\n\nFor information on how the data was gathered, please refer to the data tab on this website. There, you can find information on data gathering, cleaning, etc. For clusterization methods, our feature data would be the same as the data used for all other models, but I have to remove the *PastGroup* variable, since those are the clusters we are trying to predict.\n\nAlthough clustering analysis is often used as a data exploration method, separating the vectors into their correct *PastGroup* label would offer an appropriate predictive model for me. This is because the goal is for my model to predict who makes it out of the group phase, which would be the same as classifying them under one of the two possible groups: 0 (didn't make it) and 1 (got out of the group stage).\n\n### Model descriptions\n\n**K-Means**\n\nK-Means is a model that separates *n* vectors into *k* clusters by assigning them to the cluster whose prototype (cluster centroid) is most similar to the vector in question. In simpler terms, K-Means attempts to group different rows of data based on their similarity. Hopefully, if the grouping is done correctly, we can learn new patterns and information about our data and how they relate to each other. \n\nFor this method, we often use the Euclidean distance between points to understand their similarity. The closer the points are to each other, the more similar they are, and vice versa. To optimize this model, we use the elbow method, which is a graph that uses the inertia and/or distortion metrics to evaluate the model's ability to cluster data into different groups.\n\nEven though K-Means is a fast and efficient clustering method, it tends to work best with very specific datasets. It doesn't do well with data that isn't obviously forming a cluster or data that has unusual shapes (e.g. rings, circles, etc.).\n\n**DBSCAN**\n\nDBSCAN stands for Density Based Clustering of Applications with Noise. This type of model focuses on the density of data around an area and allows for points that are relatively nearby to be classified as different clusters if there happens to be non-dense space between them. For points that have no density around them, they are seen as outliers. This type of model is non-parametric, meaning that all adjustments to its performance come from the user in the form of hyperparameters.\n\nDBSCANs are best used for non-linear data, as it is able to better capture complicated relationships between data points. In situations where K-Means struggles to properly identify clusters, it would be a good idea to bring in DBSCANs to see if they perform better.\n\nIn order to find the optimal hyperparameters, just like with K-Means, we can use the Elbow Method or the Silhouette Method. The Elbow Method focuses on the sum of squared distances of samples to their closest cluster center (good would be low inertia and low number of clusters), while the Silhouette Method focuses on the distance between the clusters (the closer the score is to 1, the better the clustering).\n\n**Hierarchical Clustering**\n\nHierarchical clustering is not just one type of model, but a type of clustering. This method of clustering doesn't assume a particular number of groups, rather it creates a tree (dendrogram) of clusters and continues to separate points until each point is seen as its own cluster. Within hierarchical clustering, there are two types of methods: agglomerative and division. Agglomerative starts by making each point a cluster and works its way up to one cluster. Division does the opposite.\n\nThese types of models are comparable to DBSCANs in terms of performance. They are able to cluster non-linear data with good precision. However, each model's accuracy can be dependent on the situation. It is best used for EDA and occasionaly as label predictors (depending on your data). To optimize it, we can also use the Elbow and Silhouette Methods.\n\n### Data preprocessing\n\nFor this model, I had to remove certain non-numerical labels, given that numerical data is needed for clustering. I could've turned the string/factor variables into numerical data but they wouldn't provide much information on the teams' performance. The removed variables are *Country*, *Year*, and *Group*. Since none of these are seen as likely predictors of a team's success, I chose to remove them for this modeling section.\n\nThe data used will be the same as the data previously shown on this page, but I will be removing the *PastGroup* variable to use later on for evaluation purposes. Also, there is no point in leaving the respective clusters for each data point within their vector! Furthermore, I standardized the data to ensure proper representation in the vector and improve results. Following is the normalized data set.\n\n### Model creation\n\n**K-Means**\n\nFirst we must go through hyperparameter tuning. In order to do so, we use the Elbow Method, which means iterating over different, possible hyperparameters to see which one gives us the best score.\n\nElbow plot:\n\nAccording to the above graphs, the optimal choice of clusters would be at the inflection point of 5 clusters. Now that we know this, we must recreate the model with the optimal number of clusters.\n\nIdeally, we would've wanted the clustering method to group the data into two groups, one representing the teams that got into the knockout stage and the other representing the teams that didn't make it out of the group stage. Unfortunately, the optimal number of clusters is 5, so we now have to figure out what each of them represent.\n\n**DBSCAN**\n\nWe repeat the hyperparameter tuning process used for K-Means, but this time with the silhouette score.\n\nThe optimal results say that we should choose a DBSCAN model with eps of 2. For context, the eps hyperparameter defines the threshold for distance between points to be considered part of the same cluster. That is, for each point that is being compared to another, if they are less than 2 distance units away from each other, they will be considered as belonging to the same group. For the silhouette score, we want to get as close to 1 as possible, and an eps of 2.0 gave us the highest silhouette score of 0.24. This score and eps correspond to a model that created 3 clusters.\n\nDBSCAN's optimal model clusters the data into 3 groups, and most of the points are being put into two of the three groups. This result is closer to what we're looking for, but we need to see if the clusters correspond to the teams that make it out group stages and those who don't.\n\n**Agglomerative**\n\nWe repeat the hyperparameter tuning process used for DBSCAN modeling.\n\nThe optimal results say that we should choose a model with 4 clusters, as the silhouette score is highest with that option. Choosing 3 clusters would yield a similar score, so if we are interested in minimizing the number of clusters, 3 would also be a good option. Since I want to get 2 or 3 clusters, I'll go for the smaller option.\n\nThe above code shows some of the attributes coming from the optimal model.\n\n### Results\nGiven that K-Mean works best for linear data, I found DBSCAN and agglomerative clustering to be the ones that gave the best result, or at least, the results closest to what I was looking for. K-Means wanted to create too many clusters and the other two options decided on a range of 2-4 clusters as the optimal amount. In terms of ease of use, they were all equally easy to create. Since I'm using the sklearn library, all I had to do was replicate the code I had used for the previous model. It only took some messing around with the hyperparameters to create the model. \n\nTo find connections between the predicted labels and the gorund truth, I added the predictions back into the y dataframe that contained the labels and compared the values for each model's predictions against the labels.\n\nI decided to sum the rows to test the accuracy of the clustering methods. Since the length of *PastGroup* is 160, the closer the sum gets to this number (only works with the columns that end in '.D'), the closer we are to good accuracy. We can take the sum of each column and divide by 160 to get the accuracy. Based on this scoring method, the model that performs best is the DBSCAN model, but it still comes in at less than 50%. This is mainly due to the clustering methods creating more than two clusters. So, by default, we will immediately get incorrect clusterization for anything that isn't a 0 or a 1. This could be fixable by choosing hyperparameters that yield 2 clusters, but then we wouldn't be selecting the optimal models according to their processes.\n\nTo see if the clustering can give any new information on the data, I plotted a two-dimensional version of the data and colored the points based on their predicte cluster. It can be diffuclt to visualize the reasoning behind the clustering since we're not including all dimensions used to cluster.\n\nUnfortunately, the clustering did not provide any new insights on the data. The agglomerative clustering method gets a bit closer to properly separating the clusters than the two other emthods, but it's still not good enough. From this limited perspective, I theorize that the variables I have for predicting who makes it out of the group stage might not be as effective as initially thought. Either that, or some of the variables aren't needed and are just creating additional noise.\n\n### Conclusion\nI would like to start my conclusion by reminding the reader that clusterization methods are usually employed for exploratory data analysis. These methods are not meant to be used as predictive models, but, with the right data, they can work well in doing so. After using K-Means, DBSCAN, and Agglomerative clustering methods, optimal models separated the data into 3 or 4 clusters. Although the models are likely finding trends in the data we haven't been able to decipher, they aren't keen in clustering the data into only 2 categories.\n\nIf desired, we could ask the models to purposely put the data into only two clusters, and then we could compare their label predictions against the teams who made it past the group stage and those who didn't. This would be a next step for this section, but for now I want to accept the recommended optimal models as the ones to analyze.\n\nIf we're trying to predict teams' success in the World Cup with an accuracy of over 70%, these models are not the way to go. The DBSCAN, which has the best score at ~45% accuracy rate, is performing at a worse rate than a random guess. Do keep in mind that the clusters have more than 2 categories, so it makes perfect sense for them to be inefficient in classifying a 2-category problem. Going forward, it could be valuable to explore the relationships that the model found and also their ability to predict the teams that make it out of the group stage. Maybe the model's ability to predict 1s is much better than the overall accuracy!\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"message":false,"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"pdf-engine":"lualatex","toc":true,"output-file":"clustering.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":{"light":"flatly","dark":"darkly"},"title-block-banner":"../images/copa2.jpeg","title-block-banner-color":"white","toc-title":"Contents","bibliography":["../references.bib"]},"extensions":{"book":{"multiFile":true}}}}}